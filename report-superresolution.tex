\documentclass{paper}

%\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{hyperref}


% load package with ``framed'' and ``numbered'' option.
%\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}

% something NOT relevant to the usage of the package.
\setlength{\parindent}{0pt}	
\setlength{\parskip}{18pt}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\usepackage[latin1]{inputenc} 
\usepackage[T1]{fontenc} 

\usepackage{listings} 
\lstset{% 
   language=Matlab, 
   basicstyle=\small\ttfamily, 
} 



\title{Assignment 1}



\author{Moser Stefan\\09-277-013}
% //////////////////////////////////////////////////


\begin{document}



\maketitle


% Add figures:
%\begin{figure}[t]
%%\begin{center}
%\quad\quad   \includegraphics[width=1\linewidth]{ass2}
%%\end{center}
%
%\label{fig:performance}
%\end{figure}

\section*{Superresolution}

\subsection*{Problem}
Through various effects, an image may end up with a low resolution. 
This can be due to hardware limitations 
(e. g. an image taken from a cheap security camera), 
bandwith limitations 
(e. g. an image was downscaled for transmitting over a connection with very low bandwith) or others.  


\subsection*{Motivation}
To retrieve further details, we might now want to increase image quality by increasing its resolution. 
While this is not possible in a manner as depicted in \href{http://petapixel.com/2012/08/17/ridiculous-photo-enhancement-scene-from-the-tv-show-csi/}{popular TV series}, 
we can make certain assumptions for regularization combined with convex optimization to increase quality by a fair amount. 
In this project use a regularization term that penalizes high gradients,
assuming that the original image largely consists of smooth patches.

\subsection*{Derivation of Gradient}

The energy term we want to optimize in this problem is 
\begin{equation}
E(u) = \norm{\nabla u} + \frac{\lambda}{2} \norm{Du - g}^2_2.
\end{equation}
The left summand we will further call regularisation term, 
the right one fitting term. 

The regularisation term consists only of the gradient
of the superresolution image $u$, conserving the smoothness of the image.
While any norm would be acceptable, I chose the 2-norm for its nice
mathematical properties.

The fitting term consists of the channel selector $D$, 
the superresolution image $u$ and the initial image $g$ (with low resultion).
This term ensures that the optimum
\begin{equation}
\tilde{u} = \min_{u \in X} E(u) = \min_{u \in X} \norm{\nabla u} + \frac{\lambda}{2} \norm{Du - g}^2_2
\end{equation}
stays close to the initial
image when applying the channel selector. 

The fitting term on the right can be derived pretty easily using the chain rule
\begin{align}
\frac{\partial E(u)}{\partial u} 
&= \norm{\nabla u} + \frac{\lambda}{2} (Du - g)^T(Du - g) \frac{\partial}{\partial u} \\
&= \norm{\nabla u} \frac{\partial}{\partial u} + D^T D u - D^T g
\end{align}
by expressing the images $u$ and $g$ as vectors. For further use, we define
this term as 
\begin{equation}
 f = D^T D u - D^T g
\end{equation}
The regularization term however needs some more work. 
For this purpose we first discretize the norm of the gradient as
\begin{equation}
\norm{\nabla u} \approx \sum_{i,j} 
	\sqrt{(u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2}
\end{equation}
One single term of this sum can then be expressed as
\begin{equation}
\tau[i,j] = \sqrt{(u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2}
\end{equation}
Using these tools, we can now easily define the gradient of the regularization term in a single component as
\begin{equation}
\frac{\partial \norm{\nabla u}}{\partial u[i,j]} = 
	\frac{\partial \tau[i,j]}{\partial u[i,j]} +
	\frac{\partial \tau[i - 1,j]}{\partial u[i,j]} +
	\frac{\partial \tau[i,j - 1]}{\partial u[i,j]}
\end{equation}
With all other terms being independent of $u[i,j]$ and thus being zero.
These three expressions can now be computed as
\begin{align}
\frac{\partial \tau[i,j]}{\partial u[i,j]} 
&= &&\frac{\partial \sqrt{(u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2}}{\partial u[i,j]} \\
&= &&\frac{1}{2} ((u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2)^{-\frac{1}{2}} \\
& &&\frac{\partial}{\partial u[i,j]} 
(u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2 \\
&= &&\frac{1}{2}\tau[i,j]^{-1} \\
& &&\frac{\partial}{\partial u[i,j]} 
(u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2 \\
&= &&\frac{1}{2}\tau[i,j]^{-1}
(-2(u[i + 1, j] - u[i,j]) - 2(u[i, j+1] - u[i,j])) \\
&= &&\tau[i,j]^{-1} (2 u[i,j] -u[i + 1, j] - u[i, j+1]) \\
&= &&\frac{2u[i,j] - u[i + 1, j] - u[i, j+1]}{\tau[i,j]}
\end{align}
In a similar fashion, we can compute the partial derivatives for
$\tau[i-1, j]$ and $\tau[i, j - 1]$
\begin{align}
\frac{\partial \tau[i - 1,j]}{\partial u[i,j]} 
&= &&\frac{\partial \sqrt{(u[i,j] - u[i-1,j])^2 + (u[i-1, j+1] - u[i-1, j])^2}}{\partial u[i,j]} \\
&= &&\frac{1}{2} \cdot \frac{1}{\tau[i - 1, j]} \cdot 2 \cdot (u[i,j] - u[i-1, j]) \\
&= &&\frac{u[i,j] - u[i-1, j]}{\tau[i - 1, j]}
\end{align}
respectively
\begin{align}
\frac{\partial \tau[i,j-1]}{\partial u[i,j]} 
&= &&\frac{\partial \sqrt{(u[i + 1 ,j] - u[i,j - 1])^2 + (u[i, j] - u[i, j - 1])^2}}{\partial u[i,j]} \\
&= &&\frac{1}{2} \cdot \frac{1}{\tau[i, j - 1]} \cdot 2 \cdot (u[i,j] - u[i, j-1]) \\
&= &&\frac{u[i,j] - u[i, j - 1]}{\tau[i, j-1]}
\end{align}

The $\tau$ terms need additional care, since they appear in the denominator
of the fractions. To prevent division by zero and increase numerical 
stability, we add we approximate them as 
\begin{equation}
\tilde{\tau}[i,j] = \sqrt{(u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2 + \delta}
\end{equation}
with $\delta$ being a small positive number.

Combining our efforts from before, we can express the gradient of the objective function as
\begin{align}
\nabla E[i] = &\frac{u[i,j] - u[i-1, j]}{\tilde{\tau}[i - 1, j]} + \\
	&\frac{u[i,j] - u[i, j - 1]}{\tilde{\tau}[i, j-1]} + \\
	&\frac{u[i,j] - u[i, j - 1]}{\tilde{\tau}[i, j-1]} + \\
	&f[i]
\end{align}

\subsection*{Implementation}

\begin{enumerate}

\item \textbf{Implement gradient descent for superresolution.} In this section you should:

\begin{itemize}
\item Show some images, as the the gradient method progresses iteration by iteration. Display the initial and the final image and 3 more images in between.
\end{itemize}

\item \textbf{Show images obtained by very high, very low and optimal $\lambda$.} In this section you should:

\begin{itemize}
\item Display 3 images with different $\lambda$ (very low, very high and optimal).
\item Describe the effect of $\lambda$ on the solution.
\end{itemize}

\item \textbf{ Find optimal $\lambda$.} In this section you should:

\begin{itemize}
\item Display the $SSD$ vs. $\lambda$ graph.
\item Describe the effect of $\lambda$ with respect to the $SSD$ between the ground truth and the solution image.
\end{itemize}


\end{enumerate}


 \end{document}
 
 