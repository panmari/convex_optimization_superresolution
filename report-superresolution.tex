\documentclass{paper}

%\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{hyperref}


% load package with ``framed'' and ``numbered'' option.
%\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}

% something NOT relevant to the usage of the package.
\setlength{\parindent}{0pt}	
\setlength{\parskip}{18pt}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\usepackage[latin1]{inputenc} 
\usepackage[T1]{fontenc} 

\usepackage{listings} 
\lstset{% 
   language=Matlab, 
   basicstyle=\small\ttfamily, 
} 



\title{Assignment 1}



\author{Moser Stefan\\09-277-013}
% //////////////////////////////////////////////////


\begin{document}



\maketitle


% Add figures:
%\begin{figure}[t]
%%\begin{center}
%\quad\quad   \includegraphics[width=1\linewidth]{ass2}
%%\end{center}
%
%\label{fig:performance}
%\end{figure}

\section*{Superresolution}

\subsection*{Problem}
Through various effects, an image may end up with a low resolution. 
This can be due to hardware limitations 
(e. g. an image taken from a cheap security camera), 
bandwith limitations 
(e. g. an image was downscaled for transmitting over a connection with very low bandwith) or others.  


\subsection*{Motivation}
To retrieve further details, we might now want to increase image quality by increasing its resolution. 
While this is not possible in a manner as depicted in \href{http://petapixel.com/2012/08/17/ridiculous-photo-enhancement-scene-from-the-tv-show-csi/}{popular TV series}, 
we can make certain assumptions for regularization combined with convex optimization to increase quality by a fair amount. 
In this project use a regularization term that penalizes high gradients,
assuming that the original image largely consists of smooth patches.

\subsection*{Derivation of Gradient}
\begin{equation}
E(u) = \norm{\nabla u} + \frac{\lambda}{2} \norm{Du - g}^2_2
\end{equation}

Gradient

The fitting term on the right can be derived pretty easily using the chain rule for derivation.
\begin{align}
\frac{\partial E(u)}{\partial u} 
&= \norm{\nabla u} + \frac{\lambda}{2} (Du - g)^T(Du - g) \frac{\partial}{\partial u} \\
&= \norm{\nabla u} \frac{\partial}{\partial u} + D^T D u - D^T g
\end{align}

The regularization term however needs some more work. 
For this purpose we first discretize the norm of the gradient as
\begin{equation}
\norm{\nabla u} \approx \sum_{i,j} 
	\sqrt{(u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2}
\end{equation}
One single term of this sum can then be expressed as
\begin{equation}
\tau[i,j] = \sqrt{(u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2}
\end{equation}
Using these tools, we can now easily define the derivation of the regularization term as
\begin{equation}
\frac{\partial \norm{\nabla u}}{\partial u[i,j]} = 
	\frac{\partial \tau[i,j]}{\partial u[i,j]} +
	\frac{\partial \tau[i - 1,j]}{\partial u[i,j]} +
	\frac{\partial \tau[i,j - 1]}{\partial u[i,j]}
\end{equation}
With all other terms being independent of $u[i,j]$ and thus being zero.
These three expressions can now be computed as
\begin{align}
\frac{\partial \tau[i,j]}{\partial u[i,j]} 
&= &&\frac{\partial \sqrt{(u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2}}{\partial u[i,j]} \\
&= &&\frac{1}{2} ((u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2)^{-\frac{1}{2}} \\
& &&\frac{\partial}{\partial u[i,j]} 
(u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2 \\
&= &&\frac{1}{2}\tau[i,j]^{-1} \\
& &&\frac{\partial}{\partial u[i,j]} 
(u[i+1,j] - u[i,j])^2 + (u[i, j+1] - u[i, j])^2 \\
&= &&\frac{1}{2}\tau[i,j]^{-1}
(-2(u[i + 1, j] - u[i,j]) - 2(u[i, j+1] - u[i,j])) \\
&= &&\tau[i,j]^{-1} (2 u[i,j] -u[i + 1, j] - u[i, j+1]) \\
&= &&\frac{2u[i,j] - u[i + 1, j] - u[i, j+1]}{\tau[i,j]}
\end{align}
In a similar fashion, we can compute the partial derivatives for
$\tau[i-1, j]$ and $\tau[i, j - 1]$
\begin{align}
\frac{\partial \tau[i - 1,j]}{\partial u[i,j]} 
&= &&\frac{\partial \sqrt{(u[i,j] - u[i-1,j])^2 + (u[i-1, j+1] - u[i-1, j])^2}}{\partial u[i,j]} \\
&= &&\frac{1}{2} \cdot \frac{1}{\tau[i - 1, j]} \cdot 2 \cdot (u[i,j] - u[i-1, j]) \\
&= &&\frac{u[i,j] - u[i-1, j]}{\tau[i - 1, j]}
\end{align}
respectively
\begin{align}
\frac{\partial \tau[i,j-1]}{\partial u[i,j]} 
&= &&\frac{\partial \sqrt{(u[i + 1 ,j] - u[i,j - 1])^2 + (u[i, j] - u[i, j - 1])^2}}{\partial u[i,j]} \\
&= &&\frac{1}{2} \cdot \frac{1}{\tau[i, j - 1]} \cdot 2 \cdot (u[i,j] - u[i, j-1]) \\
&= &&\frac{u[i,j] - u[i, j - 1]}{\tau[i, j-1]}
\end{align}

Interestingly, all square roots are eliminated in this process.
\begin{equation}
\min_{u \in X} \norm{\nabla u} + \frac{\lambda}{2} \norm{Du - g}^2_2
\end{equation}

Discretized 
\begin{equation}
\min_{u} \sum_{i=1}^{N-1} \norm{u[i+1] - u[i]}_2 + 
\frac{\lambda}{2} \sum_{i=1}^{N-1} (Du[i] - g[i])^2
\end{equation}

Finite gradient:
TODO: show that chain rule does eliminate square root.

\begin{equation}
\nabla E[i] = 2 (2u[i] - u[i + 1] - u[i - 1]) + \lambda D[i,i] u[i] (Du[i] - g[i])
\end{equation}
\begin{enumerate}
\item \textbf{Derivation of gradient.} In this section you should:

\begin{itemize}
\item Write the finite difference approximation of the objective function $E$.
\item Compute the gradient of the objective function $\nabla_uE$.  
\end{itemize}


\item \textbf{Implement gradient descent for superresolution.} In this section you should:

\begin{itemize}
\item Show some images, as the the gradient method progresses iteration by iteration. Display the initial and the final image and 3 more images in between.
\end{itemize}

\item \textbf{Show images obtained by very high, very low and optimal $\lambda$.} In this section you should:

\begin{itemize}
\item Display 3 images with different $\lambda$ (very low, very high and optimal).
\item Describe the effect of $\lambda$ on the solution.
\end{itemize}

\item \textbf{ Find optimal $\lambda$.} In this section you should:

\begin{itemize}
\item Display the $SSD$ vs. $\lambda$ graph.
\item Describe the effect of $\lambda$ with respect to the $SSD$ between the ground truth and the solution image.
\end{itemize}


\end{enumerate}


 \end{document}
 
 